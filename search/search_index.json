{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Who I am","text":"<p>I am Kleber Yokota, passionate about data. I worked as a Data Scientist and now as a Data Engineer. The purpose of this text is to document the studies and discoveries</p> <p>I make during my free time. Since technology evolves and changes so rapidly, it\u2019s important to have a place to take notes and share them with others who have similar interests in tech.</p> <p>As a Data Engineer, I need to use many technologies these days, and having a dedicated space is crucial. My home lab serves as my playground, where I can experiment, test, and learn about different technologies. This space is not just for Data Engineering or technologies related to data infrastructure. It\u2019s a place to explore the full spectrum of tools and frameworks that deal with data, from Machine Learning Engineering to LLMs (Large Language Models) and beyond. I see the increasing need to understand the full ecosystem of data-related technologies, as they all play a role in how we process, analyze, and utilize data.</p>"},{"location":"#home-lab","title":"Home Lab","text":"<p>Why did I decide to use a PC as a home lab? Well, since I decided to become a Data Scientist, I followed many online tutorials to run my first model or study Python. There was a lot of copying and pasting on my laptop, but I had no idea what I was doing, which caused many things to break. I installed software I didn't fully understand, and I had to format my PC multiple times. In some cases, I even overheated and overloaded my laptop. From all of that, I learned that I needed a space where I could make mistakes without impacting my daily work.</p> <p>This led me to learn tools like pyenv, Docker, VMs, and more. Eventually, I needed something more robust\u2014something used in production environments. That\u2019s when I began using Kubernetes (K8s).</p> <p>Now, my home lab runs on Talos, a lightweight distribution that uses K8s and provides security by default. This setup is incredibly helpful because my goal is to create an environment that mirrors production systems. I don\u2019t need to be an expert in K8s to manage security features. Honestly, if I had a place where I could run as root without worrying too much about security, I\u2019d probably encounter fewer problems but also wouldn\u2019t learn as much. When you\u2019re forced to solve problems, you begin to understand the inner workings of the tools you're using and the systems you\u2019re building.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/","title":"ClickHouse on Kubernetes in My Homelab","text":"","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#what-is-clickhouse","title":"What is ClickHouse?","text":"<p>ClickHouse is a columnar database, highly performant and widely used for analytics and handling large datasets. It serves as the Data Warehouse in my homelab.</p> <p>To deploy ClickHouse on my Kubernetes cluster, I use the Altinity distribution, which provides an Operator and Kubernetes CRDs (Custom Resource Definitions). This Operator adds custom resources to manage ClickHouse, like the <code>chi</code> command to check cluster status.</p>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#installing-the-clickhouse-operator","title":"Installing the ClickHouse Operator","text":"","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#step-1-install-the-operator","title":"Step 1 \u2014 Install the Operator","text":"<p>Run the following command to install the Altinity Operator on Kubernetes:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/Altinity/clickhouse-operator/master/deploy/operator/clickhouse-operator-install-bundle.yaml\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#step-2-create-a-namespace","title":"Step 2 \u2014 Create a Namespace","text":"<pre><code>kubectl create namespace clickhouse-demo\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#deploying-clickhouse","title":"Deploying ClickHouse","text":"<p>Below is the YAML I use to deploy a simple ClickHouse cluster in my homelab with:</p> <ul> <li>3 shards (data partitions);</li> <li>1 replica per shard.</li> </ul>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#important-notes","title":"Important notes:","text":"<ul> <li>The <code>users</code> section defines the username and password. If you don\u2019t specify network access (<code>networks/ip</code>), it may block external connections.</li> <li>This YAML is integrated with Longhorn as the storage backend, so volumes are automatically provisioned.</li> </ul> <pre><code>apiVersion: clickhouse.altinity.com/v1\nkind: ClickHouseInstallation\nmetadata:\n  name: clickhouse-demo\n  namespace: clickhouse-demo\nspec:\n  defaults:\n    templates:\n      podTemplate: clickhouse-pod\n      volumeClaimTemplate: clickhouse-volume\n  configuration:\n    users:\n      default/password: qwerty\n      default/networks/ip: \"::/0\"\n    clusters:\n      - name: cluster1\n        layout:\n          shardsCount: 3\n          replicasCount: 1\n        templates:\n          podTemplate: clickhouse-pod\n          volumeClaimTemplate: clickhouse-volume\n  templates:\n    podTemplates:\n      - name: clickhouse-pod\n        spec:\n          containers:\n            - name: clickhouse\n              image: clickhouse/clickhouse-server:25.4\n    serviceTemplates:\n      - name: cluster1-cluster-ip\n        spec:\n          type: ClusterIP\n          ports:\n            - name: http\n              port: 8123\n              targetPort: 8123\n            - name: https\n              port: 8443\n              targetPort: 8443\n            - name: tcp\n              port: 9000\n              targetPort: 9000\n    volumeClaimTemplates:\n      - name: clickhouse-volume\n        spec:\n          accessModes: [ \"ReadWriteOnce\" ]\n          storageClassName: longhorn\n          resources:\n            requests:\n              storage: 100Gi\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#creating-a-manual-service","title":"Creating a Manual Service","text":"<p>Although the Operator creates some services automatically, I prefer creating a manual Service for better control, especially when exposing it via Traefik and securing with Let's Encrypt.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: clickhouse-clusterip\n  namespace: clickhouse-demo\nspec:\n  selector:\n    clickhouse.altinity.com/chi: clickhouse-demo\n  ports:\n    - name: http\n      port: 8123\n      targetPort: 8123\n    - name: tcp\n      port: 9000\n      targetPort: 9000\n  type: ClusterIP\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#exposing-with-traefik-and-tls","title":"Exposing with Traefik and TLS","text":"<p>In my setup, I use Traefik as the Ingress Controller with Let's Encrypt for TLS certificates. For this, I create:</p> <ul> <li>An <code>IngressRoute</code> in Traefik;</li> <li>A <code>Certificate</code> with cert-manager.</li> </ul> <p>\u26a0\ufe0f Important: Ensure your DNS points to your cluster IP, and Traefik is configured to handle the domain.</p>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#testing-the-connection","title":"Testing the Connection","text":"<p>You can test the connection to ClickHouse using Python with the <code>clickhouse-connect</code> library.</p>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#install-the-library","title":"Install the library:","text":"<pre><code>pip install clickhouse-connect\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#test-script","title":"Test script:","text":"<pre><code>from clickhouse_connect import get_client\n\nclient = get_client(\n    host='clickhouse.upliftdata.pro',\n    port=443,\n    username='default',\n    password='qwerty',\n    secure=True\n)\n\nresult = client.query('SELECT now()').result_rows\nprint(result)\n</code></pre> <p>The expected output is the current datetime, confirming the connection works.</p>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/05/22/clickhouse-on-kubernetes-in-my-homelab/#conclusion","title":"Conclusion","text":"<p>With this setup, ClickHouse is running in my Kubernetes cluster with high availability, persistent storage via Longhorn, and secure exposure using Traefik. This is one more production-grade component running in my homelab.</p>","tags":["talos","k8s","kubernetes","homelab","datastack","clickhouse"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/","title":"Talos - Standalone Installation on a Machine","text":"","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#creating-a-bootable-usb","title":"Creating a Bootable USB","text":"<p>Download the Talos ISO and create a bootable USB drive. If you're unfamiliar with creating a bootable USB, you can use tools like balenaEtcher (Windows/macOS/Linux).</p>","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#installing-talosctl","title":"Installing Talosctl","text":"<p>To install <code>talosctl</code>, run the following command:</p> <pre><code>brew install siderolabs/tap/talosctl\n</code></pre>","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#generating-configuration-files","title":"Generating Configuration Files","text":"<p>Decide on a cluster name; in this case, we'll use <code>k8s-dev</code>.</p> <p>Check the cluster's IP address or endpoint. In this example, we use <code>192.168.0.30</code>.</p> <p>Run the following command to generate the configuration files:</p> <pre><code>talosctl gen config &lt;cluster-name&gt; https://&lt;cluster-endpoint&gt;:6443\n</code></pre> <p>For our example:</p> <pre><code>talosctl gen config k8s-dev https://192.168.0.30:6443\n</code></pre> <p>This will generate three files in the current directory: <code>controlplane.yaml</code>, <code>worker.yaml</code>, and <code>talosconfig</code>.</p>","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#identifying-the-installation-disk","title":"Identifying the Installation Disk","text":"<p>To list the available disks on your machine, use the following command:</p> <pre><code>talosctl -n &lt;cluster-endpoint&gt; get disks --insecure\n</code></pre> <p>Example output: </p>","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#how-to-choose-the-correct-disk","title":"How to Choose the Correct Disk","text":"<p>Each line represents a storage device available on the system. Here\u2019s what to look for:</p> <ul> <li> <p>ID: This is the device name you will use when modifying <code>controlplane.yaml</code> (e.g., <code>nvme0n1</code>, <code>sda</code>).</p> </li> <li> <p>SIZE: Check the disk size to ensure you're selecting the correct one.</p> </li> <li> <p>TRANSPORT: Identifies whether the disk is NVMe, USB, or another type.</p> </li> <li> <p>ROTATIONAL: If <code>true</code>, it's likely an HDD; if <code>false</code>, it's an SSD or NVMe.</p> </li> <li> <p>MODEL: Displays the manufacturer and model name of the disk.</p> </li> <li> <p>READ ONLY: If <code>true</code>, the disk cannot be written to.</p> </li> </ul> <p>Important: Do not select the USB drive (e.g., <code>sda</code>) if you're installing Talos on an internal disk. Choose an NVMe or SATA SSD instead.</p> <p>After determining the target disk, modify the <code>controlplane.yaml</code> file accordingly.</p>","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#modifying-controlplaneyaml","title":"Modifying controlplane.yaml","text":"<p>Locate the section containing the disk configuration:</p> <p></p> <p>Change the disk path to <code>/dev/&lt;your-chosen-disk&gt;</code>. By default, Talos installs on the USB drive, but we want to specify a different disk.</p> <ul> <li> <p><code>image</code>: Defines the Talos version to install.</p> </li> <li> <p><code>wipe</code>: If set to <code>true</code>, the disk will be formatted before installation.</p> </li> </ul> <p>For example, to install on <code>/dev/nvme0n1</code> with disk formatting enabled, update the configuration accordingly.</p> <p></p> <p>Since using a single machine need to running workload on control-plane nodes</p> <p> Just uncomment the allowSchedulingOnControlPlanes</p>","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#installing-talos","title":"Installing Talos","text":"<p>To apply the configuration and install Talos, run:</p> <pre><code>talosctl apply-config -n &lt;cluster-endpoint&gt; --file controlplane.yaml --insecure\n</code></pre> <p>For our example:</p> <pre><code>talosctl apply-config -n 192.168.0.30 --file controlplane.yaml --insecure\n</code></pre> <p>Wait for the API to become available:</p> <pre><code>talosctl -e &lt;cluster-endpoint&gt; -n &lt;cluster-endpoint&gt; containers\n</code></pre> <p>Example:</p> <pre><code>talosctl -e 192.168.0.30 -n 192.168.0.30 containers\n</code></pre> <p>Once confirmed, apply the bootstrap process:</p> <pre><code>talosctl bootstrap -n 192.168.0.30\n</code></pre>","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#verifying-the-installation","title":"Verifying the Installation","text":"<p>To check the Talos services, run:</p> <pre><code>talosctl -e 192.168.0.30 -n 192.168.0.30 --talosconfig ./talosconfig services\n</code></pre> <p>A successful installation will show multiple running services, including <code>apid</code>, <code>containerd</code>, <code>etcd</code>, and <code>kubelet</code>.</p>","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#testing-kubernetes","title":"Testing Kubernetes","text":"","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#generating-the-kubeconfig","title":"Generating the Kubeconfig","text":"<p>Since this is the only Kubernetes cluster being used, generate the <code>kubeconfig</code> file with:</p> <pre><code>talosctl kubeconfig ./kubeconfig --nodes 192.168.0.30 --endpoints 192.168.0.30 --talosconfig=./talosconfig\n</code></pre> <p>Move the generated <code>kubeconfig</code> file to the appropriate location and test the cluster with:</p> <pre><code>kubectl get pods -A\n</code></pre>","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2024/03/12/talos---standalone-installation-on-a-machine/#testing-control-plane-with-a-simple-pod","title":"Testing Control Plane with a Simple Pod","text":"<p>To ensure the control plane is functioning properly, you can run a test pod with:</p> <pre><code>kubectl run test --restart=Never --image=hello-world\n</code></pre> <p>This will create a simple pod to verify that the Kubernetes control plane can deploy and manage workloads successfully.</p>","tags":["talos","k8s","kubernetes","homelab"]},{"location":"blog/2025/06/17/llm-agents---local-experiments-with-langgraph/","title":"LLM Agents - Local Experiments with LangGraph","text":"","tags":["LLM","ollama","langgraph","homelab","datastack","pandas"]},{"location":"blog/2025/06/17/llm-agents---local-experiments-with-langgraph/#purpose","title":"Purpose","text":"<p>This blog documents my personal experiments with building LLM-based agents, running entirely on local hardware. The main tool used is LangGraph, which allows the creation of agents as computation graphs.</p> <p>This is an ongoing learning project to explore:</p> <ul> <li>How to design LLM agents.</li> <li>How to build data generators leveraging LLM text generation.</li> <li>How to handle limitations and control outputs.</li> </ul>","tags":["LLM","ollama","langgraph","homelab","datastack","pandas"]},{"location":"blog/2025/06/17/llm-agents---local-experiments-with-langgraph/#premises","title":"Premises","text":"<ol> <li>Everything runs locally \u2014 no cloud dependency \u2014 for full control.</li> <li>Focused on building a data generator agent.</li> <li>Goal is to study how LLMs can generate structured data (tables) suitable for later use and analysis.</li> <li>Outputs include CSV files for evaluation.</li> <li>The project is in early stages (version 0) and under active development \u2014 there may be bugs or unused code.</li> <li>Screenshots and debug prints are included to help track progress.</li> <li>Initial experiments use Qwen 3 4B \u2014 to understand how smaller SLMs perform locally.</li> </ol>","tags":["LLM","ollama","langgraph","homelab","datastack","pandas"]},{"location":"blog/2025/06/17/llm-agents---local-experiments-with-langgraph/#objectives","title":"Objectives","text":"<ul> <li> <p>Understand how to design LangGraph flows:</p> </li> <li> <p>Decisions</p> </li> <li>Prompting for entity extraction</li> <li>Capturing intent from LLM responses</li> <li> <p>Test whether LLMs can consistently generate structured outputs:</p> </li> <li> <p>Number of columns</p> </li> <li>Number of rows</li> <li>Thematic consistency</li> <li>Controlled specifications</li> <li>Explore if the agent can generate metadata describing the generated data \u2014 critical for making outputs reusable in later stages.</li> <li> <p>Implement an intent node to decide:</p> </li> <li> <p>Should a new dataset be generated?</p> </li> <li>Should metadata be modified?</li> <li>Can data be reused?</li> <li>This tests LangGraph's ability to handle decision-making flows.</li> <li>Final output for each run: 10 example rows saved as CSV, to evaluate data generation quality.</li> </ul>","tags":["LLM","ollama","langgraph","homelab","datastack","pandas"]},{"location":"blog/2025/06/17/llm-agents---local-experiments-with-langgraph/#current-status-version-0","title":"Current Status (Version 0)","text":"<p>This is version 0 \u2014 the first prototype. The agent is still evolving. You will find code samples and links to the GitHub repo containing the current implementation and further updates.</p>","tags":["LLM","ollama","langgraph","homelab","datastack","pandas"]},{"location":"blog/2025/06/17/llm-agents---local-experiments-with-langgraph/#example-csv-output","title":"Example CSV Output","text":"<p>Here is a properly formatted CSV with 10 rows of data, ensuring all fields are correctly structured and adhere to the specified requirements. Each field containing commas or special characters is enclosed in double quotes to maintain CSV integrity.</p> <pre><code>\"u12345\",\"john doe\",\"johndoe@example.com\",\"p@ssw0rd!\",\"1990-05-15\",\"male\",\"+1 (212) 555-0198\",\"123 main st, springfield, il, 62704\",\"springfield\",\"illinois\",\"62704\",\"usa\",\"2023-04-05t14:30:45\",\"2023-04-06t10:15:30\",\"active\"\n\"u67890\",\"jane smith\",\"janesmith@domain.com\",\"securepass123!\",\"1985-08-22\",\"female\",\"+44 20 7946 0012\",\"456 oak ave, london, uk, sw1a 1aa\",\"london\",\"uk\",\"sw1a 1aa\",\"uk\",\"2023-04-07t09:45:22\",\"2023-04-08t14:00:15\",\"inactive\"\n\"u34567\",\"emily johnson\",\"emilyj@domain.com\",\"qwerty!123\",\"1995-03-12\",\"other\",\"+33 1 234 5678\",\"789 rue de paris, paris, france, 75001\",\"paris\",\"france\",\"75001\",\"france\",\"2023-04-09t17:20:45\",\"2023-04-10t08:30:10\",\"active\"\n\"u98765\",\"michael brown\",\"michaelb@domain.com\",\"pass123!@#\",\"1988-11-04\",\"male\",\"+1 (310) 555-1234\",\"101 pine st, new york, ny, 10001\",\"new york\",\"new york\",\"10001\",\"usa\",\"2023-04-11t13:15:30\",\"2023-04-12t09:45:20\",\"active\"\n\"u54321\",\"sarah wilson\",\"sarahw@domain.com\",\"secure@123\",\"1992-07-25\",\"female\",\"+44 20 7946 0013\",\"202 brook st, manchester, uk, m1 1aa\",\"manchester\",\"uk\",\"m1 1aa\",\"uk\",\"2023-04-13t10:55:45\",\"2023-04-14t12:10:30\",\"inactive\"\n\"u76543\",\"david miller\",\"davidm@domain.com\",\"test123!@#\",\"1980-02-10\",\"male\",\"+1 (201) 555-3333\",\"303 maple st, chicago, il, 60614\",\"chicago\",\"illinois\",\"60614\",\"usa\",\"2023-04-15t15:30:45\",\"2023-04-16t14:20:15\",\"suspended\"\n\"u23456\",\"laura evans\",\"lauree@domain.com\",\"login!123\",\"1998-09-18\",\"other\",\"+33 1 234 5679\",\"404 rue de lyon, lyon, france, 69001\",\"lyon\",\"france\",\"69001\",\"france\",\"2023-04-17t09:45:20\",\"2023-04-18t12:30:45\",\"active\"\n\"u123456\",\"robert johnson\",\"robertj@domain.com\",\"secure@pass123\",\"1994-01-05\",\"male\",\"+1 (212) 555-9876\",\"505 green st, san francisco, ca, 94101\",\"san francisco\",\"california\",\"94101\",\"usa\",\"2023-04-19t14:00:45\",\"2023-04-20t10:15:30\",\"inactive\"\n\"u654321\",\"jessica williams\",\"jessica@domain.com\",\"test@pass!123\",\"1987-05-05\",\"female\",\"+44 20 7946 0014\",\"606 river st, birmingham, uk, b1 1aa\",\"birmingham\",\"uk\",\"b1 1aa\",\"uk\",\"2023-04-21t13:55:20\",\"2023-04-22t11:45:30\",\"suspended\"\n\"u987654\",\"william davis\",\"williamd@domain.com\",\"pass123!@#\",\"1989-12-25\",\"male\",\"+33 1 234 5670\",\"707 boulevard de paris, paris, france, 75002\",\"paris\",\"france\",\"75002\",\"france\",\"2023-04-23t10:30:45\",\"2023-04-24t14:20:15\",\"active\"\n</code></pre>","tags":["LLM","ollama","langgraph","homelab","datastack","pandas"]},{"location":"blog/2025/06/17/llm-agents---local-experiments-with-langgraph/#key-notes","title":"Key Notes","text":"<ul> <li>Phone numbers: include country codes (e.g., <code>+1</code> for the US, <code>+44</code> for the UK, <code>+33</code> for France).</li> <li>Addresses: properly formatted with city, state, and ZIP/postcode.</li> <li>Dates: follow ISO 8601 format (<code>yyyy-mm-ddThh:mm:ss</code>).</li> <li>Quotes: used around fields containing commas or special characters.</li> </ul> <p>This data is ready for import into spreadsheet software or databases.</p>","tags":["LLM","ollama","langgraph","homelab","datastack","pandas"]},{"location":"blog/2025/06/17/llm-agents---local-experiments-with-langgraph/#observations","title":"Observations","text":"<ul> <li> <p>The agent currently tends to repeat certain patterns, such as:</p> </li> <li> <p>Similar email addresses with low variability.</p> </li> <li>Some repeated formats, despite temperature set to 1.0.</li> <li>Occasionally, the LLM injects extra information not matching the intended structure, causing parsing issues.</li> <li>After running multiple times and refining prompts, successful generation of valid CSVs was achieved using Pandas for post-processing.</li> <li>Improvements are ongoing to better control output variability and consistency.</li> </ul> <p>More versions and improvements will be documented in future updates.</p>","tags":["LLM","ollama","langgraph","homelab","datastack","pandas"]},{"location":"blog/2024/04/10/minio/","title":"Minio","text":"","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#minio-longhorn-traefik-cert-manager-cloudflare-on-talos-kubernetes","title":"MinIO + Longhorn + Traefik + Cert-Manager + Cloudflare on Talos Kubernetes","text":"","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#what-is-minio","title":"What is MinIO?","text":"<p>MinIO is an object storage system, just like AWS S3. In fact, it uses the same protocol. This means you can store any kind of file and access it via APIs or SDKs \u2014 a great foundation for your data stack.</p> <p>It makes a lot of sense to have an object store in your infrastructure.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#how-to-persist-data","title":"How to persist data","text":"<p>Kubernetes runs stateless containers \u2014 which means pods don\u2019t keep any data. Every time a pod is restarted or scaled, it starts fresh.</p> <p>But apps like MinIO or databases need to persist data. If you run a database pod without any volume, it will boot up empty every time.</p> <p>To solve this, you create a <code>PersistentVolume</code> and a <code>PersistentVolumeClaim</code>. This way, your app knows where to store and recover data even after restarts.</p> <p>In this setup, I\u2019ll run everything on a single node, so I won\u2019t worry about complex scheduling.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#installing-longhorn","title":"Installing Longhorn","text":"<p>To manage storage, I\u2019m using Longhorn, which automatically provisions volumes.</p> <p>Longhorn needs special privileges, so let\u2019s isolate it:</p> <pre><code>kubectl create namespace longhorn-system\nkubectl label namespace longhorn-system pod-security.kubernetes.io/enforce=privileged\n</code></pre> <p>Then, configure Talos OS to allow disk operations. This requires enabling two extensions and mounting a folder Longhorn uses.</p> <p>Use Talos Factory to build a custom image with these extensions:</p> <ul> <li><code>util-linux-tools</code></li> <li><code>iscsi-tools</code></li> </ul> <pre><code>talos upgrade --image factory.talos.dev/installer/&lt;IMAGE_HASH&gt;:v1.9.4 -m powercycle -f -e 192.168.0.30 -n 192.168.0.30\n</code></pre> <p>Warning: use only one IP for both <code>-e</code> and <code>-n</code>.</p> <p>Now update your <code>talosconfig</code>:</p> <pre><code>machine:\n  kubelet:\n    extraMounts:\n      - destination: /var/lib/longhorn\n        type: bind\n        source: /var/lib/longhorn\n        options:\n          - bind\n          - rshared\n          - rw\n</code></pre> <p>Apply it:</p> <pre><code>talosctl apply-config -f controlplane.yaml\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#installing-longhorn-via-helm","title":"Installing Longhorn via Helm","text":"<pre><code>helm repo add longhorn https://charts.longhorn.io\nhelm repo update\n</code></pre> <p>Create a <code>longhorn_values.yaml</code>:</p> <pre><code>defaultSettings:\n  defaultReplicaCount: 1\npersistence:\n  reclaimPolicy: Retain\n  defaultClassReplicaCount: 1\n</code></pre> <p>Then install:</p> <pre><code>helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --version 1.8.1 -f longhorn_values.yaml\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#setting-up-dns-tls-certificate-and-ingress-traefik-cert-manager-cloudflare","title":"Setting up DNS, TLS certificate, and Ingress (Traefik + cert-manager + Cloudflare)","text":"<p>You\u2019ll expose Longhorn with a domain like <code>longhorn.&lt;your-domain&gt;</code> pointing to your Traefik Load Balancer IP.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#certificate","title":"Certificate","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: longhorn-certificate\n  namespace: longhorn-system\nspec:\n  secretName: longhorn-certificate-prod\n  issuerRef:\n    name: cloudflare-issuer-prod\n    kind: ClusterIssuer\n  dnsNames:\n    - longhorn.&lt;your-domain&gt;\n</code></pre> <p>Wait for the certificate to be ready before applying the IngressRoute.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#ingressroute","title":"IngressRoute","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: longhorn-ingress\n  namespace: longhorn-system\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`longhorn.&lt;your-domain&gt;`)\n      kind: Rule\n      services:\n        - name: longhorn-frontend\n          port: 80\n  tls:\n    secretName: longhorn-certificate-prod\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#installing-minio","title":"Installing MinIO","text":"<pre><code>helm repo add minio https://operator.min.io/\nhelm repo update\n</code></pre> <p>Create a <code>values.yaml</code> for the operator:</p> <pre><code>operator:\n  env:\n    - name: OPERATOR_STS_ENABLED\n      value: \"on\"\n  replicaCount: 1\n  resources:\n    requests:\n      cpu: 200m\n      memory: 256Mi\n      ephemeral-storage: 500Mi\n</code></pre> <p>Then install the operator:</p> <pre><code>helm install minio minio-operator/operator -n minio -f values.yaml\n</code></pre> <p>Now install a tenant, which is the actual MinIO instance.</p> <p>Create <code>tenant.yaml</code>:</p> <pre><code>tenant:\n  name: myminio\n  image:\n    repository: quay.io/minio/minio\n    tag: RELEASE.2025-03-12T18-04-18Z\n    pullPolicy: IfNotPresent\n  configSecret:\n    name: myminio-env-configuration\n    accessKey: minio\n    secretKey: minio123\n  pools:\n    - servers: 1\n      name: pool-0\n      volumesPerServer: 1\n      size: 300Gi\n      storageClassName: longhorn\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n        fsGroupChangePolicy: \"OnRootMismatch\"\n        runAsNonRoot: true\n      containerSecurityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        runAsNonRoot: true\n        allowPrivilegeEscalation: false\n        capabilities:\n          drop:\n            - ALL\n        seccompProfile:\n          type: RuntimeDefault\n  mountPath: /export\n  subPath: /data\n  certificate:\n    requestAutoCert: false\n  exposeServices:\n    console: false\n    minio: false\n</code></pre> <p>\u26a0\ufe0f Credentials are hardcoded for now. You should extract them into a Kubernetes Secret in production.</p> <pre><code>helm install --namespace tenant-ns --create-namespace tenant minio/tenant -f tenant.yaml\n</code></pre> <p>Note: MinIO tries to generate a self-signed cert which can break health checks. We\u2019ll fix that using cert-manager.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#certificate_1","title":"Certificate","text":"<pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: minio-tenant-certificate\n  namespace: tenant-ns\nspec:\n  secretName: minio-tenant-certificate-prod\n  issuerRef:\n    name: cloudflare-issuer-prod\n    kind: ClusterIssuer\n  dnsNames:\n    - minio.&lt;your-domain&gt;\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#ingressroute_1","title":"IngressRoute","text":"<pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: minio-tenant-ingress\n  namespace: tenant-ns\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`minio.&lt;your-domain&gt;`)\n      kind: Rule\n      services:\n        - name: myminio-console\n          port: 9090\n  tls:\n    secretName: minio-tenant-certificate-prod\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/10/minio/#bonus-tip-using-multiple-nvme-disks-with-longhorn","title":"Bonus tip: using multiple NVMe disks with Longhorn","text":"<p>If you have multiple NVMe drives, you can configure Talos to use all of them with Longhorn.</p> <p>List all available disks:</p> <pre><code>talosctl get DiscoveredVolume\n</code></pre> <p>Then add this to your Talos config:</p> <pre><code>machine:\n  disks:\n    - device: /dev/nvme2n1\n      partitions:\n        - mountpoint: /var/lib/longhorn/nvme2n1/\n    - device: /dev/nvme3n1\n      partitions:\n        - mountpoint: /var/lib/longhorn/nvme3n1/\n</code></pre> <p>Apply and reboot:</p> <pre><code>talosctl apply-config -f controlplane.yaml\ntalosctl reboot\n</code></pre> <p>Now go to the Longhorn dashboard and add the new disks to each node.</p> <p>Enjoy your highly available object store setup with MinIO + Longhorn on Kubernetes!</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","datastack","minio","longhorn"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/","title":"Proof of Concept: OpenTelemetry with Python and Docker","text":"<p>This project is a technical Proof of Concept (PoC) to explore how OpenTelemetry works for tracing, logging, and metrics collection in Python applications. The goal is to understand its architecture and capabilities to enable deeper future analyses and observability improvements. github</p>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#why-this-poc","title":"Why This PoC?","text":"<p>Modern systems require deep visibility into application behavior to detect issues, optimize performance, and improve reliability. This PoC was created to:</p> <ul> <li>Learn how OpenTelemetry collects and exports telemetry data (traces, metrics, logs).</li> <li>Experiment with local telemetry ingestion and visualization using Grafana's otel-lgtm stack.</li> <li>Lay the foundation for applying observability practices to more complex, production-grade systems in the future.</li> </ul> <p>By understanding these concepts now, it will be easier to build smarter monitoring, alerting, and analysis pipelines later.</p>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#benefits-of-using-opentelemetry","title":"Benefits of Using OpenTelemetry","text":"<ul> <li>Standardization: One open-source standard for observability across vendors and technologies.</li> <li>Flexibility: Easily export telemetry data to many different backends (Grafana, Datadog, Jaeger, Prometheus, etc.).</li> <li>Rich Telemetry: Collect traces, logs, and metrics in a unified and correlated way.</li> <li>Vendor Neutrality: Avoid lock-in by instrumenting once and choosing/exporting to any compatible backend.</li> <li>Extensibility: OpenTelemetry supports advanced use cases like baggage propagation, contextual data, and auto-instrumentation.</li> </ul>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#possible-applications-of-opentelemetry","title":"Possible Applications of OpenTelemetry","text":"<p>In future projects, OpenTelemetry can be used to:</p> <ul> <li>Monitor microservices in real-time (track service-to-service communications).</li> <li>Optimize resource usage (CPU, memory, network) and detect bottlenecks automatically.</li> <li>Correlate failures across distributed systems.</li> <li>Create intelligent alerting systems based on real telemetry patterns.</li> <li>Improve system reliability by proactively detecting anomalies before users are affected.</li> <li>Enable better post-mortem analysis by combining traces, logs, and metrics into a single timeline.</li> </ul>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#about-opentelemetry","title":"About OpenTelemetry","text":"<p>OpenTelemetry is an open-source observability framework that provides APIs, SDKs, and tools to instrument, generate, collect, and export telemetry data (logs, metrics, and traces). It is a CNCF project and aims to standardize the way software systems are monitored.</p> <p>In this project, OpenTelemetry is used to:</p> <ul> <li>Trace ETL steps (<code>extract</code>, <code>transform</code>, and <code>load</code>) using spans.</li> <li>Log important messages during the ETL flow.</li> <li>Measure CPU and RAM usage dynamically during execution.</li> </ul> <p>All telemetry data is sent to a local OpenTelemetry backend deployed via Docker Compose.</p>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#project-structure","title":"Project Structure","text":"<ul> <li>Python ETL script with OpenTelemetry tracing, logging, and metrics.</li> <li>Docker Compose file to run the observability backend stack using Grafana's otel-lgtm.</li> <li>pyproject.toml to manage Python dependencies.</li> </ul>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#running-the-project","title":"Running the Project","text":"","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#1-start-the-observability-backend","title":"1. Start the Observability Backend","text":"<pre><code>docker-compose up -d\n</code></pre> <p>This will launch the following components:</p> <ul> <li>Grafana UI (port <code>3000</code>)</li> <li>Tempo (tracing backend)</li> <li>Loki (logging backend)</li> <li>Mimir (metrics backend)</li> <li>OTLP receiver (<code>4317</code> for gRPC, <code>4318</code> for HTTP)</li> </ul> <p>Grafana credentials: - User: admin - Password: admin</p> <p>Access Grafana: http://localhost:3000</p>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#2-install-python-dependencies","title":"2. Install Python Dependencies","text":"<pre><code>uv pip install --system\n</code></pre> <p>This will install the necessary libraries from <code>pyproject.toml</code>.</p>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#3-run-the-etl-script","title":"3. Run the ETL Script","text":"<pre><code>python main.py\n</code></pre> <p>This script will: - Simulate CPU-intensive work by finding large prime numbers. - Log each ETL step (extract, transform, load). - Export traces, metrics, and logs to the OpenTelemetry backend.</p>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#key-parts-of-the-code","title":"Key Parts of the Code","text":"","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#opentelemetry-setup","title":"OpenTelemetry Setup","text":"<pre><code>from opentelemetry import trace, metrics, _logs\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk._logs import LoggerProvider, LoggingHandler\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter\nfrom opentelemetry.exporter.otlp.proto.grpc._log_exporter import OTLPLogExporter\n\n# Tracer, Meter, and Logger providers configured\n</code></pre>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#etl-simulation-functions","title":"ETL Simulation Functions","text":"<pre><code>def extract():\n    with tracer.start_as_current_span(\"extract\") as span:\n        stress_cpu()\n        logger.info(\"Extract step completed\")\n        span.set_status(Status(StatusCode.OK))\n\ndef transform():\n    with tracer.start_as_current_span(\"transform\") as span:\n        stress_cpu()\n        logger.info(\"Transform step completed\")\n        span.set_status(Status(StatusCode.OK))\n\ndef load():\n    with tracer.start_as_current_span(\"load\") as span:\n        time.sleep(random.uniform(1, 2))\n        logger.info(\"Load step completed\")\n        span.set_status(Status(StatusCode.OK))\n</code></pre>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#cpu-and-ram-metrics","title":"CPU and RAM Metrics","text":"<pre><code>def cpu(span_id):\n    def callback(options):\n        process = psutil.Process()\n        yield Observation(process.cpu_percent(interval=0.5), {\"span_id\": span_id})\n    return callback\n\ndef ram(span_id):\n    def callback(options):\n        process = psutil.Process()\n        yield Observation(process.memory_info().rss / (1024 * 1024), {\"span_id\": span_id})\n    return callback\n</code></pre>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#observability-architecture","title":"Observability Architecture","text":"<pre><code>[Python ETL Script] -&gt; [OpenTelemetry SDK] -&gt; [OTLP gRPC Exporter] -&gt; [otel-lgtm stack (Tempo, Loki, Mimir, Grafana)]\n</code></pre>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#future-improvements","title":"Future Improvements","text":"<ul> <li>Add attributes and events to spans.</li> <li>Include exception tracking.</li> <li>Explore baggage propagation across services.</li> <li>Use a distributed system (multiple microservices) for a more realistic setup.</li> </ul>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/04/28/proof-of-concept-opentelemetry-with-python-and-docker/#conclusion","title":"Conclusion","text":"<p>This PoC gives a foundational understanding of how OpenTelemetry can instrument a Python application and export telemetry data to observability tools.</p> <p>Repository: https://github.com/your-username/poc-opentelemetry</p> <p>Author: Your Name</p> <p>License: MIT</p>","tags":["docker","docker-compose","POC","telemetry"]},{"location":"blog/2024/03/26/tailscale-operator/","title":"Tailscale Operator","text":"","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/26/tailscale-operator/#motivation","title":"Motivation","text":"<p>I\u2019m setting up a homelab on a single machine, and ensuring it is both secure and part of the Tailscale network is crucial. The goal is to create an environment that closely resembles a production setup. By leveraging Tailscale, I can secure all the services in my homelab while maintaining a network that mimics the connectivity and security requirements of real-world production systems. This setup ensures that my infrastructure remains protected while providing the flexibility and scalability that I might need as the homelab evolves.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/26/tailscale-operator/#setup","title":"Setup","text":"","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/26/tailscale-operator/#helm-to-install-helm","title":"Helm To install Helm","text":"<p>To install Helm, run the following command:</p> <pre><code>brew install helm\n</code></pre> <p>Helm helps to install various components in Kubernetes, saving you from manually creating numerous YAML configuration files. You can add the Tailscale repository like this:</p> <pre><code>helm repo add tailscale https://pkgs.tailscale.com/helmcharts\n</code></pre> <p>This command adds all necessary files to deploy Tailscale. Then, update the repository:</p> <pre><code>helm repo update\n</code></pre> <p>It's always a good idea to keep the files up to date.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/26/tailscale-operator/#tailscale","title":"Tailscale","text":"<p>In the Tailscale admin console, add the following part to the <code>Access Control</code> section:</p> <pre><code>\"tagOwners\": {\n        \"tag:k8s-operator\": [],\n        \"tag:k8s\":          [\"tag:k8s-operator\"],\n    },\n</code></pre> <p>After that, go to <code>Settings</code> -&gt; <code>OAuth Client</code> to generate an OAuth client with write access for <code>Devices Core</code> and <code>Auth Keys</code>. Add the tag <code>tag:k8s-operator</code> for both.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/26/tailscale-operator/#installation","title":"Installation","text":"","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/26/tailscale-operator/#tailscale-with-helm","title":"Tailscale with Helm","text":"<p>First, create the namespace and apply the configuration:</p> <pre><code>kubectl create namespace tailscale\nkubectl label namespace tailscale pod-security.kubernetes.io/enforce=privileged\n</code></pre> <p>The first command creates the namespace, which is how Kubernetes organizes and applies group configurations. The second command enforces privileged mode in the namespace, which is required since Talos has strict security settings and doesn't allow certain privileges that Tailscale needs.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/26/tailscale-operator/#why-this-is-necessary","title":"Why This Is Necessary:","text":"<p>Talos enforces strict security policies, and for the Tailscale operator to function correctly, it requires certain privileges that are restricted in default configurations. The <code>pod-security.kubernetes.io/enforce=privileged</code> label ensures that the Tailscale operator has the necessary permissions to run.</p> <p>Next, run the following command in the terminal, replacing the OAuth ID and secret provided by Tailscale:</p> <pre><code>helm upgrade \\\n  --install \\\n  tailscale-operator \\\n  tailscale/tailscale-operator \\\n  --namespace=tailscale \\\n  --create-namespace \\\n  --set-string oauth.clientId=\"&lt;OAuth client ID&gt;\" \\\n  --set-string oauth.clientSecret=\"&lt;OAuth client secret&gt;\" \\\n  --wait\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/26/tailscale-operator/#explanation","title":"Explanation:","text":"<p>This command installs the Tailscale operator in the <code>tailscale</code> namespace. It configures the OAuth client ID and secret, which are necessary for authentication and authorization with Tailscale. The <code>--wait</code> flag ensures that Helm waits until the deployment is complete before returning.</p> <p>Check if the <code>tailscale-operator</code> is connected on the machines.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/26/tailscale-operator/#final-test-verify-nginx-ip-in-tailscale","title":"Final Test: Verify NGINX IP in Tailscale","text":"<p>Use the following configuration to expose the NGINX service with Tailscale:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:latest\n          ports:\n            - containerPort: 80\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    tailscale.com/expose: \"true\"\nspec:\n  selector:\n    app: nginx\n  ports:\n    - port: 80\n      targetPort: 80\n</code></pre> <p>Now apply the configuration:</p> <pre><code>kubectl create namespace demo-tailscale\nkubectl apply -f tailscale_ingress.yaml  -n demo-tailscale\n</code></pre> <p>This configuration allows the Tailscale operator to detect the service and assign an IP within the Tailscale VPC, enabling secure communication across the network.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/26/tailscale-operator/#key-point","title":"Key Point:","text":"<p>Don\u2019t forget to include this annotation in the service:</p> <pre><code>annotations:\n  tailscale.com/expose: \"true\"\n</code></pre> <p>This allows the Tailscale operator to detect the service and assign an IP within the Tailscale VPC, enabling secure communication across the network.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/","title":"Talos - Adding Features and Tips","text":"","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#what-is-a-vpc-and-why-companies-use-it","title":"What is a VPC and Why Companies Use It?","text":"<p>A Virtual Private Cloud (VPC) is an isolated, private network within a public cloud provider or an on-premises environment. It allows secure communication between resources without exposing them to the public internet.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#why-companies-use-vpcs","title":"Why Companies Use VPCs?","text":"<ul> <li> <p>Security: Resources inside a VPC are isolated and protected from external access.</p> </li> <li> <p>Private Networking: Services can communicate internally without needing public IPs.</p> </li> <li> <p>Scalability: Easily expand the infrastructure while maintaining security policies.</p> </li> <li> <p>Remote Access: With the right setup (e.g., VPN or Tailscale), teams can securely manage internal services.</p> </li> </ul>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#why-use-a-vpc-in-my-homelab","title":"Why Use a VPC in My Homelab?","text":"<p>In my homelab, I aim to create an environment that closely resembles a production setup. Since I\u2019m often on the go, I need reliable and secure remote access to manage my infrastructure from anywhere.</p> <p>Instead of exposing public IPs\u2014which increases security risks like unauthorized access and attacks\u2014I use Tailscale to build a secure, encrypted VPC over the internet. This allows me to connect to my Talos cluster seamlessly, just as if I were on the local network, while keeping everything private and protected.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#adding-tailscale-access-to-the-cluster","title":"Adding Tailscale Access to the Cluster","text":"<p>Talos supports extensions, and one of them is Tailscale. With this, we can create a VPC and access the cluster remotely.</p> <p>To add a new capability, you need to create a custom Talos image with Tailscale using Talos Factory. Once generated, the factory provides an image ID that you will use to install and upgrade Talos.</p> <p></p> <p>This new image will be used to upgrade Talos.</p> <p></p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#upgrade-talos-with-the-new-image","title":"Upgrade Talos with the New Image","text":"<p>Run the following command to upgrade Talos with the new image:</p> <pre><code>talosctl upgrade --image factory.talos.dev/installer/4a0d65c669d46663f377e7161e50cfd570c401f26fd9e7bda34a0216b6f1922b:v1.9.4 -m powercycle -f -e 192.168.0.30 -n 192.168.0.30\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#verify-tailscale-extension-installation","title":"Verify Tailscale Extension Installation","text":"<p>To check if the Tailscale extension is installed, use:</p> <pre><code>talosctl -e 192.168.0.30 -n 192.168.0.30 get extensions\n</code></pre> <p>Expected output:</p> <pre><code>NODE           NAMESPACE   TYPE              ID   VERSION   NAME        VERSION\n192.168.0.30   runtime     ExtensionStatus   0    1         tailscale   1.78.1\n192.168.0.30   runtime     ExtensionStatus   1    1         schematic   xxxxxxxxxxxxxxxxxxxxxxxxx\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#generate-and-apply-tailscale-auth-key","title":"Generate and Apply Tailscale Auth Key","text":"<p>In Tailscale, generate an authentication key. Then, add it to <code>tailscale.patch.yaml</code>:</p> <pre><code>---\napiVersion: v1alpha1\nkind: ExtensionServiceConfig\nname: tailscale\nenvironment:\n  - TS_AUTHKEY=&lt;tailscale auth key&gt;\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#enable-tailscale","title":"Enable Tailscale","text":"<p>Apply the configuration:</p> <pre><code>talosctl apply-config -n 192.168.0.30 -e 192.168.0.30 --file controlplane.yaml -p @tailscale.patch.yaml\n</code></pre> <p>Check the Tailscale admin panel to verify the cluster\u2019s IP.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#update-control-plane-configuration","title":"Update Control Plane Configuration","text":"<p>To allow Talos to receive commands via Tailscale, update <code>certSANs</code> with the Tailscale IP and reapply the configuration:</p> <pre><code>talosctl apply-config -n 192.168.0.30 -e 192.168.0.30 --file controlplane.yaml -p @tailscale.patch.yaml\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#tips","title":"Tips","text":"","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#1-setting-nodes-and-endpoints","title":"1. Setting Nodes and Endpoints","text":"<p>Typing the endpoint and nodes every time for <code>talosctl</code> can be tedious. If you have two IPs (one local and one from Tailscale), you can store them in the Talos config:</p> <pre><code>talosctl config endpoint 192.168.0.30\ntalosctl config nodes 192.168.0.30\n</code></pre> <p>However, running this command again will overwrite the previous configuration instead of adding the Tailscale IP. To keep both the local and Tailscale IPs, you need to manually edit the Talos config file and add the Tailscale IP under <code>endpoints</code> and <code>nodes</code>:</p> <pre><code>contexts:\n    k8s-dev:\n        endpoints:\n            - 192.168.0.30\n            - &lt;Tailscale IP&gt;\n        nodes:\n            - 192.168.0.30\n            - &lt;Tailscale IP&gt;\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#2-applying-configurations-properly","title":"2. Applying Configurations Properly","text":"<p>If you forget to include <code>-p @tailscale.patch.yaml</code> in the <code>apply-config</code> command, the control plane won\u2019t merge with the Tailscale configuration, leaving your cluster offline in the Tailscale network.</p> <p>Also, ensure you use <code>---</code> in YAML files to separate and merge multiple configurations correctly.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/03/20/talos---adding-features-and-tips/#3-updating-kubectl-config","title":"3. Updating <code>**kubectl**</code> Config","text":"<p>Update your <code>kubectl</code> config to use the Tailscale IP instead of the local IP.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale"]},{"location":"blog/2024/04/03/traefik/","title":"Traefik","text":"<p>Traefik is a reverse proxy, an alternative to Nginx. I found it interesting and decided to use it for my home lab. But why use a reverse proxy when Tailscale already provides a built-in DNS for devices connected to the network? The problem is that the assigned hostname is random and ends with <code>.ts.net</code>, making it difficult to remember. Additionally, remembering the IP addresses of applications is not practical.</p> <p>Since Tailscale offers MagicDNS, which allows all connected devices to share a common domain and subdomain, I can leverage my own DNS and use a reverse proxy to simplify access. This setup also enables me to obtain TLS certificates for better security.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","traefik"]},{"location":"blog/2024/04/03/traefik/#installing-traefik","title":"Installing Traefik","text":"<pre><code>helm repo add traefik https://traefik.github.io/charts\nhelm repo update\n</code></pre> <p>Create a file named <code>values.yaml</code>. This file overrides the default chart values to configure Traefik as needed.</p> <pre><code>service:  \n  type: LoadBalancer  \n  spec:  \n    loadBalancerClass: tailscale\n\nports:  \n  web:  \n    redirectTo:  \n      port: websecure\n</code></pre> <p>To deploy Traefik:</p> <pre><code>helm install traefik traefik/traefik -n traefik --create-namespace --values values.yaml\n</code></pre> <p>This will ensure that the Traefik service gets an IP address within the Tailscale network. You can then check which IP address has been assigned.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","traefik"]},{"location":"blog/2024/04/03/traefik/#testing-traefik-with-nginx","title":"Testing Traefik with Nginx","text":"<p>To test if Traefik is working, I'll deploy an Nginx instance:</p> <pre><code>apiVersion: apps/v1  \nkind: Deployment  \nmetadata:  \n  name: nginx  \n  labels:  \n    app: nginx  \nspec:  \n  replicas: 1  \n  selector:  \n    matchLabels:  \n      app: nginx  \n  template:  \n    metadata:  \n      labels:  \n        app: nginx  \n    spec:  \n      containers:  \n        - name: nginx  \n          image: nginx:latest  \n          ports:  \n            - containerPort: 80  \n\n---  \napiVersion: v1  \nkind: Service  \nmetadata:  \n  name: nginx  \n\nspec:  \n  type: ClusterIP  \n  selector:  \n    app: nginx  \n  ports:  \n    - port: 80  \n      targetPort: 80  \n\n---  \napiVersion: traefik.io/v1alpha1  \nkind: IngressRoute  \nmetadata:  \n  name: nginx-ingress  \n  namespace: default  \nspec:  \n  entryPoints:  \n    - websecure  \n  routes:  \n    - match: Host(`nginx.upliftdata.pro`)  \n      kind: Rule  \n      services:  \n        - name: nginx  \n          port: 80  \n</code></pre> <p>When connected to the network, you can use the Tailscale-provided URL to access the service.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","traefik"]},{"location":"blog/2024/04/03/traefik/#cloudflare-configuration","title":"Cloudflare Configuration","text":"<p>In Cloudflare's DNS settings, go to your domain, navigate to DNS, and set the proxy status to DNS only. Add the subdomain and the Tailscale IP assigned to the Traefik service. You can find this IP in the Tailscale dashboard.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","traefik"]},{"location":"blog/2024/04/03/traefik/#cert-manager-for-tls-certificates","title":"Cert-Manager for TLS Certificates","text":"<p>Since I have my own domain managed via Cloudflare, I can use it within Tailscale. I will also use Cloudflare for DNS and Let's Encrypt for issuing TLS certificates.</p> <p>To generate TLS certificates, Cloudflare requires an API token with the following permissions: - Zone: Read - DNS: Edit</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","traefik"]},{"location":"blog/2024/04/03/traefik/#installing-cert-manager","title":"Installing Cert-Manager","text":"<p>First, add the Cert-Manager Helm repository:</p> <pre><code>helm repo add jetstack https://charts.jetstack.io --force-update\n</code></pre> <p>Then, create a <code>cert_manager_values.yaml</code> file:</p> <pre><code>namespace: cert-manager  \ncrds:  \n  enabled: true  \nextraArgs:  \n  - --dns01-recursive-nameservers-only  \n  - --dns01-recursive-nameservers=1.1.1.1:53,1.0.0.1:53\n</code></pre> <p>Install Cert-Manager using Helm:</p> <pre><code>helm install cert-manager jetstack/cert-manager -n cert-manager --create-namespace --values cert_manager_values.yaml\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale","traefik"]},{"location":"blog/2024/04/03/traefik/#creating-the-secret-for-cloudflare-api-token","title":"Creating the Secret for Cloudflare API Token","text":"<p>Remember to encode the token in Base64 before adding it to the secret:</p> <pre><code>apiVersion: v1  \nkind: Secret  \nmetadata:  \n    name: cloudflare-api-key-secret  \n    namespace: cert-manager  \ntype: Opaque  \ndata:  \n    api-key: &lt;base64_encoded_token&gt;\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale","traefik"]},{"location":"blog/2024/04/03/traefik/#configuring-a-clusterissuer-for-lets-encrypt","title":"Configuring a ClusterIssuer for Let's Encrypt","text":"<p>The <code>ClusterIssuer</code> defines how certificates will be issued using Let's Encrypt:</p> <pre><code>apiVersion: cert-manager.io/v1  \nkind: ClusterIssuer  \nmetadata:  \n  name: cloudflare-issuer  \n  namespace: cert-manager  \nspec:  \n  acme:  \n    server: https://acme-v02.api.letsencrypt.org/directory  \n    email: &lt;your_cloudflare_email&gt;  \n    privateKeySecretRef:  \n      name: cloudflare-issuer-account-key  \n    solvers:  \n    - dns01:  \n        cloudflare:  \n          email: &lt;your_cloudflare_email&gt;  \n          apiTokenSecretRef:  \n            name: cloudflare-api-key-secret  \n            key: api-key\n</code></pre> <p>This configuration points to the production Let's Encrypt environment. If needed, a separate <code>ClusterIssuer</code> for the staging environment can be created for testing purposes.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","traefik"]},{"location":"blog/2024/04/03/traefik/#issuing-a-tls-certificate","title":"Issuing a TLS Certificate","text":"<p>To generate the actual TLS certificate, create a <code>Certificate</code> resource:</p> <pre><code>apiVersion: cert-manager.io/v1  \nkind: Certificate  \nmetadata:  \n  name: nginx-certificate  \n  namespace: default  \nspec:  \n  secretName: nginx-certificate-prod  \n  issuerRef:  \n    name: cloudflare-issuer  \n    kind: ClusterIssuer  \n  dnsNames:  \n    - &lt;your_domain_ex: nginx.example.com&gt;\n</code></pre>","tags":["talos","k8s","kubernetes","homelab","tailscale","traefik"]},{"location":"blog/2024/04/03/traefik/#updating-the-ingressroute-for-tls","title":"Updating the IngressRoute for TLS","text":"<p>Now, update the <code>IngressRoute</code> to use the generated certificate:</p> <pre><code>apiVersion: traefik.io/v1alpha1  \nkind: IngressRoute  \nmetadata:  \n  name: nginx-ingress  \n  namespace: default  \nspec:  \n  entryPoints:  \n    - websecure  \n  routes:  \n    - match: Host(`nginx.upliftdata.pro`)  \n      kind: Rule  \n      services:  \n        - name: nginx  \n          port: 80  \n  tls:  \n    secretName: nginx-certificate-prod\n</code></pre> <p>This ensures that the Nginx service is accessible securely using a TLS certificate issued via Let's Encrypt, managed by Cert-Manager, and proxied through Traefik.</p>","tags":["talos","k8s","kubernetes","homelab","tailscale","traefik"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/datastack/","title":"Datastack","text":""},{"location":"blog/category/llm/","title":"LLM","text":""},{"location":"blog/category/infrastructure/","title":"Infrastructure","text":""},{"location":"blog/category/poc/","title":"POC","text":""}]}